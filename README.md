# ECE60131_Project

Reference: [![CITRIS](assets/citris-banner.png)](https://arxiv.org/abs/2202.03169)

## Preliminaries and Causal Assumptions
- The underlying latent causal process is a dynamic Bayesian network (DBN) $G = (V,E)$ over a set of $K$ causal variables.
  - Each node $i \in V$ is associated with a causal variable $C_i$, which can be scalar or vector valued.
  - Each edge $(i,j)\in E$ represents a causal replation from $C_i$ to $C_j$: $C_i \rightarrow C_j$, where $C_i$ is a parent of $C_j$ and $pa_G (C_i)$ are all parents of $C_i$ in $G.$
- The DBN is first-order Markov, stationary, and without instantaneous effects.
  - We denote the set of all causal variables at time $t$ as $C^t = (C^t_1, \ldots, C^t_K)$, where $C^{t}$ inherits all edges from its components $C_{i}$ for $i \in \[ 1..K\]$ without introducing cycles.
  - In this setting the structure of the graph is time-invariant, i.e., $pa_G(C^t_i) = pa_G(C^1_i)$ for any $t \in \[ 1..T \]$.
  - For $t \in \[ 1, \dots , T\]$ and for each causal factor $i \in \[ 1, \dots , K \]$, we can model $C_i = f_i(pa_G (C_i^t), \epsilon_i)$, wehre $pa_G (C_i^t) \subset {C_1^{t-1}, \dots, C_k^{t-1}}.$, where all $\epsilon_{i}$ for $i \in \[ 1..K \]$ are mutually independent noises

- We use a binary intervention vector $I^{t} \in \lbrace 0,1 \rbrace^{K}$ to indicate that a variable $C_{i}^{t}$ in $G$ is intervened upon if and only if $I_{i}^{t} = 1$.
- We consider that the intervention vector components $I_{i}^{t}$ might be confounded by another $I_{j}^{t}$, $i \neq j$, and represent these dependencies with an unobserved regime variable $R^{t}$

- With this, we construct an augmented DAG $G' = (V', E')$, where $V' = \lbrace \lbrace C_i^t \rbrace\^K_{i=1} \cup \lbrace I_i^t \rbrace^K_{i=1} \cup R^t \rbrace^T_{t=1}$
  and $E' = \lbrace \lbrace pa_G(C^t_i) \to C_i^t \rbrace^{K}\_{i=1} \cup \lbrace I_i^t \to C_i^t\rbrace\^K_{i=1}
      \cup \lbrace R^{t} \to I_{i}^{t} \rbrace_{i=1}^{K} \rbrace \_{t=1}^{T}.$

- We say that a distribution $p$ is Markov w.r.t. the augmented DAG $G'$ if it factors as $p(V') = \prod_{j \in V'} p(V_{j} \mid pa_{G'}(V_{j})),$
where $V_{j}$ includes the causal factors $C_{i}^{t}$, the intervention vector components $I_{i}^{t}$, and the regime $R^{t}$. Moreover, we say that $p$ is faithful to a causal graph $G'$, if there are no additional conditional independences to the d-separations one can read from the graph $G'$.

- We will consider \emph{soft} interventions, in which the conditional distribution changes, i.e.,
  $p(C_{i}^{t} \mid pa_G(C_{i}^{t}), I_{i}^{t}=1) \neq p(C_{i}^{t} \mid pa_G(C_{i}^{t}), I_{i}^{t}=0),$
which include as a special case \emph{perfect} interventions $\mathrm{do}(C_{i} = c_{i})$

## Identifiability of minimal Causal Variables

![alt text](https://github.com/phlippe/CITRIS/blob/main/figures/icitris_setup.png)

### TempoRal Intervened Sequences (TRIS)
(we consider data generated by an underlying latent temporal causal process with $K$ \emph{causal factors} $\bigl(C_{1}^{t}, C_{2}^{t}, \ldots, C_{K}^{t}\bigr)_{t=1}^{T}$.)
- At each time step $t$, we observe a high-dimensional observation $X^{T}$ representing a noisy, entangled view of all causal factors.

#### Multidimensional Causal Factors
- We allow them to be potentially multidimensional, i.e., $C_{i} \in \mathcal{D}\_{i}^{M_{i}}$ with $M_{i} \ge 1$ and in practice we let $\mathcal{D}_{i}$ be $\mathbb{R}$ for continuous variables (e.g., spatial position), $\mathbb{Z}$ for discrete variables (e.g., the score of a player) or mixed. 
- We define the causal factor space as $\mathcal{C} = \mathcal{D}\_{1}^{M_{1}} \times \mathcal{D}\_{2}^{M_{2}} \times \cdots \times \mathcal{D}\_{K}^{M_{K}}.$

#### Observation Function
- We define the observation function $h(C_{1}^{t}, C_{2}^{t}, \ldots, C_{K}^{t}, E_{\mathrm{o}}^{t}) = X^{t}$, where $E_{\mathrm{o}}^{t}$ represents any noise independent of the causal factors that influence the observations, and $h : \mathcal{C} \times \mathcal{E} \to \mathcal{X}$ is a function from the causal factor space $\mathcal{C}$ and the space of the noise variables $\mathcal{E}$ to the observation space $\mathcal{X}$.
- We assume that $h$ is bijective, implying that the joint dimensionality of the noise and causal model is limited to the image size.
  (This allows us to identify each causal factor uniquely from observations by learning an approximation of $f$, while disregarding irrelevant features in the observation space.)

#### Availability of Intervention Targets
- We assume that in each time-step some causal factors might (or might not) have been intervened upon and that we have access to the corresponding intervention targets, but not the intervention values.

### Necessary Condition for Disentanglement in TRIS
(In TRIS, we generally cannot disentangle two causal factors if they are always intervened upon jointly, or, on the contrary, if they are never intervened upon.)

\newtheorem{prop}{Proposition}
In TRIS, if two causal factors $C_i$ and $C_j$ have only been jointly intervened on or not at all, then there exists a causal graph in which $C_i$ and $C_j$ cannot be uniquely identified from observations $X$ and intervention targets $I$.
\end{prop}

(Additionally, in TRIS where the latent causal factors may correspond to multidimensional vectors, we cannot even completely reconstruct said factors, when by the nature of the system the provided interventions leave some of the causal factorâ€™s dimensions unaffected. In the next section, we will instead introduce the concept of minimal causal variables to characterize what we can identify instead.)

### Minimal Causal Variables

![alt text](https://github.com/phlippe/CITRIS/blob/main/figures/ball_in_boxes.png)

Over time, the ball can move freely within the box it is currently in, but it can only jump into another box if there is an intervention. The intervention moves the ball to the other box, but keeps the relative position of the ball within the box intact. (While one could define this process by a single causal variable $x$ over time,)It can be described by two causal variables: the relative position within the box $x'$ and the current box $b$. Since only $b$ is affected by the intervention, and we consider causal factors to potentially be multidimensional, we could not identify which causal factor $x'$ belongs to.

(We formalize this intuition as follows.)

Suppose for each causal factor $C_i \in \mathcal{D}\_i^{M_i}$, there exists an invertible map $s_i : \mathcal{D}\_i^{M_i} \to \mathcal{D}\_i^{\mathrm{var}} \times \mathcal{D}\_i^{\mathrm{inv}}$ that splits the domain $\mathcal{D}_i^{M_i}$ of $C_i$ into a part that is variant and a part thatis invariant under intervention. We denote the two parts of this map as
```math
  s_i(C_i^{t})
  = \bigl(s_i^{\mathrm{var}}(C_i^{t}),
          s_i^{\mathrm{inv}}(C_i^{t})\bigr)
```

- The split $s$ must be invertible, so that we can map back and forth between $\mathcal{D}_i^{M_i}$ and $\mathcal{D}_i^{\mathrm{var}} \times \mathcal{D}\_i^{\mathrm{inv}}$ without losing information.
- To be called a split in this setup, $s$ must satisfy
```math
  s_i^{\mathrm{inv}}(C_i^{t}) \perp I_i^t \mid pa(C_i^t),
```
  i.e., $s_i^{\mathrm{inv}}(C_i^{t})$ is independent of the intervention variable $I_i^{t}$ given the parents of $C_i^{t}$. Also, both parts of the split must be conditionally independent, i.e.
  ```math
  s_i^{\mathrm{inv}}(C_i^{t}) \perp s_i^{\mathrm{var}}(C_i^{t}) \mid pa(C_i^{t}), I_i^{t}.
  ```
  
This means that $s_i^{\mathrm{var}}(C_i^{t})$ will contain the manipulable, or variable, part of $C_i^{t}$. In contrast, $s_i^{\mathrm{inv}}(C_i^{t})$ is the invariable part of $C_i^{t}$ which is independent of the intervention.

For any causal variable, there may exist multiple possible splits. There is always at least the trivial split where $\mathcal{D}_i^{\mathrm{var}} = \mathcal{D}_i^{M_i}$ is the original domain of $C_i$, and $\mathcal{D}_i^{\mathrm{inv}} = \{0\}$ is the one-element set (no invariant information).

(But not all splits are trivial: For the example in Figure~3, we can split the causal factor $x$ in $s_i^{\mathrm{var}}(x)$ such that the box identifier $b$ is in $s_i^{\mathrm{var}}(x)$ and the relative position in the box $x'$ is in $s_i^{\mathrm{inv}}(x)$. 

Intuitively, we want to identify the split where $s_i^{\mathrm{var}}$ contains only the manipulable information:)

\newtheorem{def}{Definition}
The minimal causal split of a variable $C_i^{t}$ with respect to its intervention variable $I_i^{t}$ is the split $s_i$ which maximizes the information content $H\bigl(s_i^{\mathrm{inv}}(C_i^{t}) \mid \mathrm{pa}(C_i^{t})\bigr)$. Under this split, $s_i^{\mathrm{var}}(C_i^{t})$ is defined as the minimal causal variable and denoted by $s_i^{\mathrm{var}*}(C_i^{t})$.
\end{def}

- $H$ denotes the entropy in the discrete case, or the limiting density of discrete points (LDDP) which measures information content of a continuous variable and is invariant under the change of variables induced by the map $s$.
(Intuitively, this ensures that only the information which truly depends on the intervention is represented in $s_i^{\mathrm{var}}(C_i)$. The definition of the minimal causal split
depends on the characteristics of the provided intervention. In our previous example, when an intervention on $x$ would also change the internal box position, the minimal causal variable would contain the full causal factor $x$. Our goal becomes to identify these minimal causal variables.)

(Depending on the interventions, the causal graph among the minimal causal graph may differ from the original graph on $C_1,\ldots,C_K$.)
- If the interventions are soft, the minimal causal variable, $s_i^{\mathrm{var}}(C_i)$, has as parents the subset of $\mathrm{pa}(C_i)$, whose relation to $C_i$ is influenced by the intervention.
(For instance, consider a three-dimensional causal variable, of which each dimension has a different set of parents. If an intervention only affects the first dimension, $s_i^{\mathrm{var}}(C_i)$ has the same parents as the first dimension, and $s_i^{\mathrm{inv}}(C_i)$ the same parents as the last two dimensions.
- For perfect interventions, the intervention-invariant part, $s_i^{\mathrm{inv}}(C_i)$, has no parents since all temporal dependencies are influenced by the intervention. Thus, in this case, the parents of $s_i^{\mathrm{var}}(C_i)$ are the same as of the true causal variable, $\mathrm{pa}(C_i)$.

### Learning Minimal Causal Variables

- We consider a dataset $\mathcal{D}$ of tuples $\{x^{t}, x^{t+1}, I^{t+1}\}$ where $x^{t}, x^{t+1} \in \mathbb{R}^{N}$ represent the observations at time step $t$ and $t+1$ respectively, and $I^{t+1}$ describes the targets of the interventions performed on $C^{t+1}$. 
- We consider a latent space $\mathcal{Z}$ larger than the latent causal factor space $\mathcal{C}$, i.e., $\mathcal{Z} \subseteq \mathbb{R}^{M}$, $M \ge \dim(\mathcal{E}) + \sum_{i=1}^{K} M_{i} = \dim(\mathcal{E}) + \dim(\mathcal{C})$. In this latent space, we aim to disentangle the causal factors. Since we may not know the exact dimensions $M_{1},\ldots,M_{K}$, we overestimate its size in the latent space $\mathcal{Z}$.
- Our goal is to approximate the inverse of the observation function $h$ by learning two components.
  - First, we learn an invertible mapping from observations to latent space, $g_{\theta} : \mathcal{X} \rightarrow \mathcal{Z}$.
  - Second, we learn an assignment function $\psi : [1..M] \rightarrow [0..K]$ that maps each dimension of $\mathcal{Z}$ to a causal factor.
  - In addition to the $K$ causal factors, we use $\psi(j) = 0$, $j \in [1..M]$ to indicate that the latent dimension $z_{j}$ does not belong to any minimal causal variable. Instead, those dimensions might model $s_{i}^{\mathrm{inv}}(C_{i})$ for some causal factor $C_{i}$; or the observation noise $E_{\mathrm{o}}^{t}$.
  - Finally, we denote the set of latent variables that $\psi$ assigns to the causal factor $C_{i}$ with $z_{\psi_{i}} = \{ z_{j} \mid j \in [1..M], \psi(j) = i \}$.
- To enforce a disentanglement of causal factors, we model a prior distribution in latent space, $p_{\phi}(z^{t+1} \mid z^{t}, I^{t+1})$, with $x^{t}, x^{t+1} \in \mathcal{X}$, $z^{t}, z^{t+1} \in \mathcal{Z}$, $z^{t} = g_{\theta}(x^{t})$, $z^{t+1} = g_{\theta}(x^{t+1})$. This transition prior enforces a disentanglement by conditioning each latent variable on exactly one of the intervention targets:
```math
p_{\phi}(z^{t+1} \mid z^{t}, I^{t+1})
= \prod_{i=0}^{K}
  p_{\phi}\bigl(z^{t+1}_{\psi_{i}} \mid z^{t}, I^{t+1}_{i}\bigr),
```
where $I^{t+1}\_{0} = 0$. Then, the objective of the model is to maximize the likelihood:
```math
p_{\phi,\theta}(x^{t+1} \mid x^{t}, I^{t+1})
  = \left\lvert
      \frac{\partial g_{\theta}(x^{t+1})}{\partial x^{t+1}}
    \right\vert
    p_{\phi}(z^{t+1} \mid z^{t}, I^{t+1}).
```

(Under the assumptions stated in Section~3.1, we can prove the following identifiability result for this setup)
- \textbf{Theorem 3.3.}
\textit{Suppose that $\phi^{\ast}$, $\theta^{\ast}$ and $\psi^{\ast}$ are the parameters that, under the constraint of maximizing the likelihood $p_{\phi,\theta}(x^{t+1} \mid x^{t}, I^{t+1})$, maximize the information content of $p_{\phi}(z^{t+1}_{\psi_{0}} \mid z^{t})$. Then, with sufficient latent dimensions, the model $\phi^{\ast}$, $\theta^{\ast}$, $\psi^{\ast}$ learns a latent structure where $z^{t+1}_{\psi_{i}}$ models the minimal causal variable of $C_{i}$ if $C_{i}^{t+1} \not\!\perp I_{i}^{t+1} \mid C^{t}, I_{j}^{t+1}$ for any $i \neq j$. All remaining information is modeled in $z_{\psi_{0}}$.}

It relies on $I_{i}^{t+1}$ not being a deterministic function of any other intervention target. A sufficient condition for this is the interventions being independent of each other, or single-target interventions with observational data. Finding the minimal variables intuitively means that the latent variables $z_{\psi_{i}}$ model only the information of $C_{i}$ which strictly depends on the intervention target $I_{i}^{t+1}$, thus defining causal variables by their intervention dependency. Going back to the example of the $x$ position of the ball in Figure~3, we would model the box identifier in $z_{\psi_{1}}$ if $C_{1} = x$, while the internal box position is modeled in $z_{\psi_{0}}$. We empirically verify the learned split for this example in Appendix~D.3. Nonetheless, one can always ensure that for any definition of the causal process, $z_{\psi_{i}}$ only contains information of causal variable $C_{i}$ of that process, and no other causal variable.

## Causal Identifiability from Temporal Intervened Sequences



